"""
TIMELY-Bench-Core æ ¸å¿ƒæ•°æ®é›†æ„å»ºå™¨

ç›®æ ‡ï¼šä»74K+æ‚£è€…ä¸­ç­›é€‰2000-5000ä¸ªé«˜è´¨é‡episodes

è´¨é‡æ ‡å‡†ï¼š
1. data_quality_score >= 0.6 (æ—¶åºæ•°æ®å®Œæ•´æ€§)
2. n_patterns >= 20 (ä¸´åºŠæ¨¡å¼ä¸°å¯Œåº¦)
3. has_aligned_spans = True (æœ‰ä¸´åºŠæ–‡æœ¬)
4. ç–¾ç—…æ ‡ç­¾è¦†ç›–å¹³è¡¡ (Sepsis/AKI/ARDS/æ­£å¸¸)
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import random
from tqdm import tqdm

# ==========================================
# é…ç½®
# ==========================================

ROOT_DIR = Path(__file__).parent
COHORT_FILE = ROOT_DIR / 'merge_output' / 'cohort_final.csv'
TIMESERIES_FILE = ROOT_DIR / 'timeseries.csv'
NOTE_TIME_FILE = ROOT_DIR / 'note_time.csv'
PATTERNS_FILE = ROOT_DIR / 'pattern_detection' / 'detected_patterns_24h.csv'
ALIGNMENT_FILE = ROOT_DIR / 'temporal_alignment' / 'temporal_textual_alignment.csv'  # å…³é”®ï¼šalignmentæ•°æ®

OUTPUT_DIR = ROOT_DIR / 'episodes_core'
SAMPLE_DIR = ROOT_DIR / 'episodes_sample'

# è´¨é‡é˜ˆå€¼
MIN_QUALITY_SCORE = 0.55  # ç•¥å¾®æ”¾å®½ä»¥è·å–è¶³å¤Ÿæ ·æœ¬
MIN_PATTERNS = 15         # è‡³å°‘15ä¸ªä¸´åºŠæ¨¡å¼
MIN_VITAL_COVERAGE = 0.7  # è‡³å°‘70%æ—¶é—´ç‚¹æœ‰ç”Ÿå‘½ä½“å¾

# ç›®æ ‡æ•°é‡
TARGET_EPISODES = 3000    # ç›®æ ‡3000ä¸ªé«˜è´¨é‡episodes


# ==========================================
# è´¨é‡è¯„ä¼°å™¨
# ==========================================

@dataclass
class EpisodeQuality:
    """Episodeè´¨é‡è¯„ä¼°ç»“æœ"""
    stay_id: int

    # æ—¶åºæ•°æ®è´¨é‡
    vital_coverage: float       # ç”Ÿå‘½ä½“å¾è¦†ç›–ç‡
    lab_coverage: float         # å®éªŒå®¤æ£€æŸ¥è¦†ç›–ç‡
    n_timepoints: int           # æ—¶é—´ç‚¹æ•°é‡

    # æ¨¡å¼ä¸°å¯Œåº¦
    n_patterns: int             # æ£€æµ‹åˆ°çš„æ¨¡å¼æ•°
    n_unique_patterns: int      # å”¯ä¸€æ¨¡å¼ç±»å‹æ•°
    n_severe_patterns: int      # ä¸¥é‡æ¨¡å¼æ•°

    # æ–‡æœ¬å¯¹é½ (æ ¸å¿ƒï¼šä½¿ç”¨alignmentæ•°æ®)
    n_notes: int                # ä¸´åºŠç¬”è®°æ•°
    n_alignments: int           # æ—¶åº-æ–‡æœ¬å¯¹é½æ•° (å…³é”®æŒ‡æ ‡)
    has_alignment: bool         # æ˜¯å¦æœ‰alignmentæ•°æ®
    has_radiology: bool         # æœ‰æ”¾å°„å­¦æŠ¥å‘Š
    has_nursing: bool           # æœ‰æŠ¤ç†ç¬”è®°

    # æ ‡ç­¾ä¿¡æ¯
    has_sepsis: bool
    has_aki: bool
    has_ards: bool
    mortality: int

    # ç»¼åˆè¯„åˆ†
    quality_score: float = 0.0

    def calculate_score(self) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ† (0-1) - ä¼˜å…ˆalignmentæ•°æ®"""
        score = 0.0

        # æ—¶åºè´¨é‡ (30%) - é™ä½æƒé‡ä»¥ç»™alignmentæ›´å¤šç©ºé—´
        score += 0.15 * min(self.vital_coverage, 1.0)
        score += 0.08 * min(self.lab_coverage, 1.0)
        score += 0.07 * min(self.n_timepoints / 24, 1.0)

        # æ¨¡å¼ä¸°å¯Œåº¦ (30%)
        score += 0.12 * min(self.n_patterns / 50, 1.0)
        score += 0.10 * min(self.n_unique_patterns / 10, 1.0)
        score += 0.08 * min(self.n_severe_patterns / 5, 1.0)

        # æ—¶åº-æ–‡æœ¬å¯¹é½ (40%) - å…³é”®æŒ‡æ ‡ï¼Œæé«˜æƒé‡
        if self.has_alignment:
            score += 0.20  # æœ‰alignmentæ•°æ®ç›´æ¥åŠ åˆ†
            score += 0.10 * min(self.n_alignments / 100, 1.0)  # alignmentæ•°é‡
        score += 0.05 * min(self.n_notes / 5, 1.0)
        score += 0.03 if self.has_radiology else 0
        score += 0.02 if self.has_nursing else 0

        self.quality_score = round(score, 3)
        return self.quality_score


class QualityAnalyzer:
    """è´¨é‡åˆ†æå™¨"""

    def __init__(self):
        self.cohort_df = None
        self.timeseries_df = None
        self.notes_df = None
        self.patterns_df = None
        self.alignment_df = None  # æ–°å¢ï¼šalignmentæ•°æ®
        self.alignment_patient_set = set()  # æœ‰alignmentæ•°æ®çš„æ‚£è€…é›†åˆ

    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("Loading data for quality analysis...")

        if COHORT_FILE.exists():
            self.cohort_df = pd.read_csv(COHORT_FILE)
            self.cohort_df['stay_id'] = self.cohort_df['stay_id'].astype(int)
            print(f"   Cohort: {len(self.cohort_df)} patients")

        if TIMESERIES_FILE.exists():
            self.timeseries_df = pd.read_csv(TIMESERIES_FILE)
            self.timeseries_df['stay_id'] = self.timeseries_df['stay_id'].astype(int)
            print(f"   Timeseries: {len(self.timeseries_df)} records")
            print(f"      Unique patients: {self.timeseries_df['stay_id'].nunique()}")

        if NOTE_TIME_FILE.exists():
            self.notes_df = pd.read_csv(NOTE_TIME_FILE)
            if 'stay_id' in self.notes_df.columns:
                self.notes_df['stay_id'] = self.notes_df['stay_id'].astype(int)
            print(f"   Notes: {len(self.notes_df)} notes")
            print(f"      Unique patients: {self.notes_df['stay_id'].nunique()}")

        if PATTERNS_FILE.exists():
            self.patterns_df = pd.read_csv(PATTERNS_FILE)
            self.patterns_df['stay_id'] = self.patterns_df['stay_id'].astype(int)
            print(f"   Patterns: {len(self.patterns_df)} patterns")
            print(f"      Unique patients: {self.patterns_df['stay_id'].nunique()}")

        # å…³é”®ï¼šåŠ è½½alignmentæ•°æ®
        if ALIGNMENT_FILE.exists():
            self.alignment_df = pd.read_csv(ALIGNMENT_FILE)
            self.alignment_df['stay_id'] = self.alignment_df['stay_id'].astype(int)
            self.alignment_patient_set = set(self.alignment_df['stay_id'].unique())
            print(f"   Alignment: {len(self.alignment_df)} alignments")
            print(f"      Unique patients with alignment: {len(self.alignment_patient_set)}")
        else:
            print(f"   Alignment file not found: {ALIGNMENT_FILE}")

    def evaluate_patient(self, stay_id: int) -> Optional[EpisodeQuality]:
        """è¯„ä¼°å•ä¸ªæ‚£è€…çš„è´¨é‡"""

        # è·å–cohortä¿¡æ¯
        patient = self.cohort_df[self.cohort_df['stay_id'] == stay_id]
        if len(patient) == 0:
            return None
        patient = patient.iloc[0]

        # æ—¶åºæ•°æ®
        if self.timeseries_df is not None:
            patient_ts = self.timeseries_df[
                (self.timeseries_df['stay_id'] == stay_id) &
                (self.timeseries_df['hour'] < 24)
            ]
            n_timepoints = len(patient_ts)

            # ç”Ÿå‘½ä½“å¾è¦†ç›–ç‡
            vital_cols = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'spo2']
            if len(patient_ts) > 0 and all(c in patient_ts.columns for c in vital_cols):
                vital_coverage = 1 - patient_ts[vital_cols].isna().mean().mean()
            else:
                vital_coverage = 0.0

            # å®éªŒå®¤è¦†ç›–ç‡
            lab_cols = ['creatinine', 'potassium', 'sodium', 'wbc', 'hemoglobin']
            if len(patient_ts) > 0 and all(c in patient_ts.columns for c in lab_cols):
                lab_coverage = 1 - patient_ts[lab_cols].isna().mean().mean()
            else:
                lab_coverage = 0.0
        else:
            n_timepoints = 0
            vital_coverage = 0.0
            lab_coverage = 0.0

        # æ¨¡å¼æ£€æµ‹
        if self.patterns_df is not None:
            patient_patterns = self.patterns_df[self.patterns_df['stay_id'] == stay_id]
            n_patterns = len(patient_patterns)
            n_unique_patterns = patient_patterns['pattern_name'].nunique() if len(patient_patterns) > 0 else 0
            n_severe_patterns = len(patient_patterns[patient_patterns['severity'] == 'severe']) if 'severity' in patient_patterns.columns and len(patient_patterns) > 0 else 0
        else:
            n_patterns = 0
            n_unique_patterns = 0
            n_severe_patterns = 0

        # ä¸´åºŠç¬”è®°
        if self.notes_df is not None and 'stay_id' in self.notes_df.columns:
            patient_notes = self.notes_df[self.notes_df['stay_id'] == stay_id]
            n_notes = len(patient_notes)

            if 'note_type' in patient_notes.columns:
                note_types = patient_notes['note_type'].str.lower().tolist()
                has_radiology = any('radiol' in str(t) for t in note_types)
                has_nursing = any('nurs' in str(t) for t in note_types)
            elif 'category' in patient_notes.columns:
                note_types = patient_notes['category'].str.lower().tolist()
                has_radiology = any('radiol' in str(t) for t in note_types)
                has_nursing = any('nurs' in str(t) for t in note_types)
            else:
                has_radiology = n_notes > 0  # å‡è®¾æœ‰ç¬”è®°å°±æœ‰æ”¾å°„å­¦
                has_nursing = False
        else:
            n_notes = 0
            has_radiology = False
            has_nursing = False

        # å…³é”®ï¼šæ—¶åº-æ–‡æœ¬å¯¹é½æ•°æ® (æ ¸å¿ƒæŒ‡æ ‡)
        has_alignment = stay_id in self.alignment_patient_set
        n_alignments = 0
        if has_alignment and self.alignment_df is not None:
            patient_alignments = self.alignment_df[self.alignment_df['stay_id'] == stay_id]
            n_alignments = len(patient_alignments)
            # ä»alignmentæ•°æ®è¡¥å……notesä¿¡æ¯
            if 'note_type' in patient_alignments.columns:
                align_note_types = patient_alignments['note_type'].str.lower().tolist()
                has_radiology = has_radiology or any('radiol' in str(t) for t in align_note_types)
                has_nursing = has_nursing or any('nurs' in str(t) for t in align_note_types)
            # è¡¥å……n_notes
            if 'note_id' in patient_alignments.columns:
                unique_notes = patient_alignments['note_id'].nunique()
                n_notes = max(n_notes, unique_notes)

        # æ ‡ç­¾ä¿¡æ¯
        has_sepsis = bool(patient.get('has_sepsis', False) or patient.get('has_sepsis_final', False))
        has_aki = bool(patient.get('has_aki', False) or patient.get('has_aki_final', False))
        has_ards = bool(patient.get('has_ards', False))
        mortality = int(patient.get('label_mortality', 0)) if pd.notna(patient.get('label_mortality')) else 0

        quality = EpisodeQuality(
            stay_id=stay_id,
            vital_coverage=round(vital_coverage, 3),
            lab_coverage=round(lab_coverage, 3),
            n_timepoints=n_timepoints,
            n_patterns=n_patterns,
            n_unique_patterns=n_unique_patterns,
            n_severe_patterns=n_severe_patterns,
            n_notes=n_notes,
            n_alignments=n_alignments,  # æ–°å¢
            has_alignment=has_alignment,  # æ–°å¢
            has_radiology=has_radiology,
            has_nursing=has_nursing,
            has_sepsis=has_sepsis,
            has_aki=has_aki,
            has_ards=has_ards,
            mortality=mortality
        )
        quality.calculate_score()

        return quality

    def analyze_all(self, sample_size: Optional[int] = None,
                     prioritize_alignment: bool = True) -> List[EpisodeQuality]:
        """åˆ†ææ‰€æœ‰æˆ–é‡‡æ ·æ‚£è€… - ä¼˜å…ˆæœ‰alignmentæ•°æ®çš„æ‚£è€…"""

        # è·å–æœ‰æ—¶åºæ•°æ®çš„æ‚£è€…
        if self.timeseries_df is not None:
            all_stay_ids = set(self.timeseries_df['stay_id'].unique())
        else:
            all_stay_ids = set(self.cohort_df['stay_id'].unique())

        # å…³é”®æ”¹è¿›ï¼šä¼˜å…ˆé€‰æ‹©æœ‰alignmentæ•°æ®çš„æ‚£è€…
        if prioritize_alignment and self.alignment_patient_set:
            alignment_ids = list(all_stay_ids & self.alignment_patient_set)
            other_ids = list(all_stay_ids - self.alignment_patient_set)

            print(f"\nPatient distribution:")
            print(f"   With alignment data: {len(alignment_ids)}")
            print(f"   Without alignment data: {len(other_ids)}")

            if sample_size:
                # ä¼˜å…ˆé€‰æ‹©æœ‰alignmentçš„æ‚£è€…
                n_alignment = min(len(alignment_ids), int(sample_size * 0.9))  # 90%æ¥è‡ªalignment
                n_other = min(len(other_ids), sample_size - n_alignment)

                stay_ids = random.sample(alignment_ids, n_alignment) if len(alignment_ids) >= n_alignment else alignment_ids
                if n_other > 0 and other_ids:
                    stay_ids.extend(random.sample(other_ids, min(n_other, len(other_ids))))
            else:
                # å…ˆåˆ†æalignmentæ‚£è€…ï¼Œå†åˆ†æå…¶ä»–
                stay_ids = alignment_ids + other_ids
        else:
            stay_ids = list(all_stay_ids)
            if sample_size and sample_size < len(stay_ids):
                stay_ids = random.sample(stay_ids, sample_size)

        print(f"\nAnalyzing {len(stay_ids)} patients...")

        qualities = []
        for stay_id in tqdm(stay_ids, desc="Evaluating"):
            q = self.evaluate_patient(stay_id)
            if q:
                qualities.append(q)

        return qualities

    def generate_report(self, qualities: List[EpisodeQuality]) -> Dict:
        """ç”Ÿæˆè´¨é‡åˆ†ææŠ¥å‘Š"""

        if not qualities:
            return {"error": "No quality data"}

        df = pd.DataFrame([vars(q) for q in qualities])

        report = {
            "total_patients": len(df),

            # è´¨é‡åˆ†æ•°åˆ†å¸ƒ
            "quality_score": {
                "mean": round(df['quality_score'].mean(), 3),
                "median": round(df['quality_score'].median(), 3),
                "std": round(df['quality_score'].std(), 3),
                "min": round(df['quality_score'].min(), 3),
                "max": round(df['quality_score'].max(), 3),
                "above_0.5": int((df['quality_score'] >= 0.5).sum()),
                "above_0.55": int((df['quality_score'] >= 0.55).sum()),
                "above_0.6": int((df['quality_score'] >= 0.6).sum()),
                "above_0.7": int((df['quality_score'] >= 0.7).sum()),
            },

            # æ—¶åºæ•°æ®
            "timeseries": {
                "avg_vital_coverage": round(df['vital_coverage'].mean(), 3),
                "avg_lab_coverage": round(df['lab_coverage'].mean(), 3),
                "avg_timepoints": round(df['n_timepoints'].mean(), 1),
            },

            # æ¨¡å¼ä¸°å¯Œåº¦
            "patterns": {
                "avg_patterns": round(df['n_patterns'].mean(), 1),
                "avg_unique_patterns": round(df['n_unique_patterns'].mean(), 1),
                "avg_severe_patterns": round(df['n_severe_patterns'].mean(), 1),
                "with_patterns_15+": int((df['n_patterns'] >= 15).sum()),
                "with_patterns_30+": int((df['n_patterns'] >= 30).sum()),
            },

            # æ–‡æœ¬è¦†ç›–
            "text_coverage": {
                "avg_notes": round(df['n_notes'].mean(), 1),
                "with_notes": int((df['n_notes'] > 0).sum()),
                "with_radiology": int(df['has_radiology'].sum()),
                "with_nursing": int(df['has_nursing'].sum()),
            },

            # æ—¶åº-æ–‡æœ¬å¯¹é½ (æ ¸å¿ƒæŒ‡æ ‡)
            "alignment_coverage": {
                "with_alignment": int(df['has_alignment'].sum()),
                "alignment_rate": round(df['has_alignment'].mean(), 3),
                "avg_alignments": round(df['n_alignments'].mean(), 1),
                "with_50+_alignments": int((df['n_alignments'] >= 50).sum()),
            },

            # ç–¾ç—…åˆ†å¸ƒ
            "disease_distribution": {
                "sepsis": int(df['has_sepsis'].sum()),
                "aki": int(df['has_aki'].sum()),
                "ards": int(df['has_ards'].sum()),
                "mortality": int(df['mortality'].sum()),
                "sepsis_rate": round(df['has_sepsis'].mean(), 3),
                "aki_rate": round(df['has_aki'].mean(), 3),
            },

            # é«˜è´¨é‡å€™é€‰ (æ ¸å¿ƒæŒ‡æ ‡ï¼šå¿…é¡»æœ‰alignmentæ•°æ®)
            "high_quality_candidates": {
                "quality>=0.55_and_patterns>=15": int(
                    ((df['quality_score'] >= 0.55) & (df['n_patterns'] >= 15)).sum()
                ),
                "quality>=0.55_and_patterns>=15_and_alignment": int(
                    ((df['quality_score'] >= 0.55) & (df['n_patterns'] >= 15) & (df['has_alignment'] == True)).sum()
                ),
                "quality>=0.6_and_patterns>=20": int(
                    ((df['quality_score'] >= 0.6) & (df['n_patterns'] >= 20)).sum()
                ),
                "quality>=0.6_and_patterns>=20_and_alignment": int(
                    ((df['quality_score'] >= 0.6) & (df['n_patterns'] >= 20) & (df['has_alignment'] == True)).sum()
                ),
                "quality>=0.6_and_patterns>=20_and_notes>=1": int(
                    ((df['quality_score'] >= 0.6) & (df['n_patterns'] >= 20) & (df['n_notes'] >= 1)).sum()
                ),
                "with_alignment_and_50+_alignments": int(
                    ((df['has_alignment'] == True) & (df['n_alignments'] >= 50)).sum()
                ),
            }
        }

        return report


# ==========================================
# æ ¸å¿ƒæ•°æ®é›†æ„å»ºå™¨
# ==========================================

class CoreDatasetBuilder:
    """æ ¸å¿ƒæ•°æ®é›†æ„å»ºå™¨"""

    def __init__(self, analyzer: QualityAnalyzer):
        self.analyzer = analyzer
        self.selected_episodes = []

    def select_episodes(self, qualities: List[EpisodeQuality],
                        target_size: int = TARGET_EPISODES,
                        require_alignment: bool = True) -> List[EpisodeQuality]:
        """é€‰æ‹©é«˜è´¨é‡episodesï¼Œä¿æŒç–¾ç—…åˆ†å¸ƒå¹³è¡¡

        Args:
            qualities: è´¨é‡è¯„ä¼°ç»“æœåˆ—è¡¨
            target_size: ç›®æ ‡æ•°é‡
            require_alignment: æ˜¯å¦å¼ºåˆ¶è¦æ±‚æœ‰alignmentæ•°æ® (æ ¸å¿ƒçº¦æŸ)
        """

        # æ ¸å¿ƒè¿‡æ»¤ï¼šä¼˜å…ˆé€‰æ‹©æœ‰alignmentæ•°æ®çš„æ‚£è€…
        if require_alignment:
            alignment_qualities = [q for q in qualities if q.has_alignment]
            no_alignment_qualities = [q for q in qualities if not q.has_alignment]
            print(f"\nAlignment filter:")
            print(f"   With alignment: {len(alignment_qualities)}")
            print(f"   Without alignment: {len(no_alignment_qualities)}")
            # ä¼˜å…ˆä½¿ç”¨æœ‰alignmentçš„æ‚£è€…
            sorted_qualities = sorted(alignment_qualities, key=lambda x: x.quality_score, reverse=True)
        else:
            sorted_qualities = sorted(qualities, key=lambda x: x.quality_score, reverse=True)

        # åˆ†å±‚é‡‡æ ·ç­–ç•¥
        selected = []

        # ç›®æ ‡åˆ†å¸ƒ (æŒ‰ç–¾ç—…ç»„)
        target_distribution = {
            'sepsis_only': target_size * 0.25,      # 25% ä»…Sepsis
            'aki_only': target_size * 0.20,         # 20% ä»…AKI
            'sepsis_aki': target_size * 0.15,       # 15% Sepsis+AKI
            'ards': target_size * 0.10,             # 10% ARDS
            'mortality': target_size * 0.10,        # 10% æ­»äº¡
            'normal': target_size * 0.20,           # 20% æ­£å¸¸/å…¶ä»–
        }

        groups = defaultdict(list)
        for q in sorted_qualities:
            if q.quality_score < MIN_QUALITY_SCORE or q.n_patterns < MIN_PATTERNS:
                continue

            # åˆ†ç»„
            if q.mortality == 1:
                groups['mortality'].append(q)
            elif q.has_ards:
                groups['ards'].append(q)
            elif q.has_sepsis and q.has_aki:
                groups['sepsis_aki'].append(q)
            elif q.has_sepsis:
                groups['sepsis_only'].append(q)
            elif q.has_aki:
                groups['aki_only'].append(q)
            else:
                groups['normal'].append(q)

        print("\nGroup sizes (filtered by quality + alignment):")
        for group, items in groups.items():
            print(f"   {group}: {len(items)}")

        # ä»æ¯ç»„é€‰æ‹©
        for group, target in target_distribution.items():
            available = groups[group]
            n_select = min(int(target), len(available))
            selected.extend(available[:n_select])
            print(f"   Selected {n_select} from {group}")

        # å¦‚æœè¿˜æ²¡è¾¾åˆ°ç›®æ ‡ï¼Œä»å‰©ä½™æœ‰alignmentçš„æ‚£è€…ä¸­è¡¥å……
        all_candidates = [q for q in sorted_qualities
                         if q.quality_score >= MIN_QUALITY_SCORE
                         and q.n_patterns >= MIN_PATTERNS
                         and q not in selected]

        remaining = target_size - len(selected)
        if remaining > 0 and all_candidates:
            selected.extend(all_candidates[:remaining])
            print(f"   Supplemented {min(remaining, len(all_candidates))} from remaining candidates")

        self.selected_episodes = selected

        # æ‰“å°alignmentç»Ÿè®¡
        n_with_alignment = sum(1 for q in selected if q.has_alignment)
        print(f"\nSelected {len(selected)} episodes")
        print(f"   With alignment: {n_with_alignment} ({n_with_alignment/len(selected)*100:.1f}%)")

        return selected

    def get_selection_summary(self) -> Dict:
        """è·å–é€‰æ‹©æ‘˜è¦"""
        if not self.selected_episodes:
            return {}

        df = pd.DataFrame([vars(q) for q in self.selected_episodes])

        return {
            "total_selected": len(df),
            "quality_score_range": [round(df['quality_score'].min(), 3),
                                   round(df['quality_score'].max(), 3)],
            "avg_quality_score": round(df['quality_score'].mean(), 3),
            "disease_distribution": {
                "sepsis": int(df['has_sepsis'].sum()),
                "aki": int(df['has_aki'].sum()),
                "ards": int(df['has_ards'].sum()),
                "mortality": int(df['mortality'].sum()),
            },
            "avg_patterns": round(df['n_patterns'].mean(), 1),
            "avg_notes": round(df['n_notes'].mean(), 1),
            # æ–°å¢ï¼šalignmentç»Ÿè®¡ (æ ¸å¿ƒæŒ‡æ ‡)
            "alignment_stats": {
                "with_alignment": int(df['has_alignment'].sum()),
                "alignment_rate": round(df['has_alignment'].mean(), 3),
                "avg_alignments": round(df['n_alignments'].mean(), 1),
                "max_alignments": int(df['n_alignments'].max()),
            },
        }

    def export_selection(self, output_file: Path):
        """å¯¼å‡ºé€‰ä¸­çš„stay_ids"""
        stay_ids = [q.stay_id for q in self.selected_episodes]

        df = pd.DataFrame([vars(q) for q in self.selected_episodes])
        df.to_csv(output_file, index=False)

        print(f"Exported {len(stay_ids)} episodes to {output_file}")


# ==========================================
# ä¸»å‡½æ•°
# ==========================================

def main():
    print("=" * 70)
    print("TIMELY-Bench-Core Dataset Builder")
    print("=" * 70)

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = QualityAnalyzer()
    analyzer.load_data()

    # åˆ†ææ‰€æœ‰æœ‰æ—¶åºæ•°æ®çš„æ‚£è€…
    print("\n" + "=" * 70)
    print("Phase 1: Quality Analysis")
    print("=" * 70)

    # å…ˆé‡‡æ ·åˆ†æï¼Œäº†è§£åˆ†å¸ƒ
    print("\nSampling 5000 patients for initial analysis...")
    sample_qualities = analyzer.analyze_all(sample_size=5000)

    sample_report = analyzer.generate_report(sample_qualities)

    print("\nğŸ“ˆ Sample Quality Report:")
    print("-" * 50)
    print(f"Total sampled: {sample_report['total_patients']}")
    print(f"\nQuality Score Distribution:")
    qs = sample_report['quality_score']
    print(f"  Mean: {qs['mean']}, Median: {qs['median']}")
    print(f"  >=0.5: {qs['above_0.5']}, >=0.55: {qs['above_0.55']}, >=0.6: {qs['above_0.6']}")

    print(f"\nPattern Richness:")
    pt = sample_report['patterns']
    print(f"  Avg patterns: {pt['avg_patterns']}")
    print(f"  With 15+ patterns: {pt['with_patterns_15+']}")
    print(f"  With 30+ patterns: {pt['with_patterns_30+']}")

    print(f"\nText Coverage:")
    tc = sample_report['text_coverage']
    print(f"  Avg notes: {tc['avg_notes']}")
    print(f"  With notes: {tc['with_notes']}")
    print(f"  With radiology: {tc['with_radiology']}")

    print(f"\nDisease Distribution:")
    dd = sample_report['disease_distribution']
    print(f"  Sepsis: {dd['sepsis']} ({dd['sepsis_rate']*100:.1f}%)")
    print(f"  AKI: {dd['aki']} ({dd['aki_rate']*100:.1f}%)")
    print(f"  ARDS: {dd['ards']}")
    print(f"  Mortality: {dd['mortality']}")

    print(f"\nHigh-Quality Candidates:")
    hq = sample_report['high_quality_candidates']
    for key, count in hq.items():
        print(f"  {key}: {count}")

    # ä¿å­˜æŠ¥å‘Š
    report_file = ROOT_DIR / 'quality_analysis_report.json'
    with open(report_file, 'w') as f:
        json.dump(sample_report, f, indent=2)
    print(f"\nReport saved to {report_file}")

    # å¦‚æœæ ·æœ¬è¶³å¤Ÿå¥½ï¼Œè¿›è¡Œå…¨é‡åˆ†æå¹¶é€‰æ‹©
    print("\n" + "=" * 70)
    print("Phase 2: Full Analysis & Selection")
    print("=" * 70)

    # ä¼°ç®—éœ€è¦åˆ†æå¤šå°‘æ‰èƒ½è·å¾—è¶³å¤Ÿçš„é«˜è´¨é‡æ ·æœ¬
    high_quality_rate = hq['quality>=0.55_and_patterns>=15'] / sample_report['total_patients']
    estimated_needed = int(TARGET_EPISODES / max(high_quality_rate, 0.1)) + 1000

    print(f"\nğŸ“ Estimated high-quality rate: {high_quality_rate*100:.1f}%")
    print(f"ğŸ“ Need to analyze approximately: {estimated_needed} patients")

    # è¿›è¡Œæ›´å¤§è§„æ¨¡åˆ†æ
    analysis_size = min(estimated_needed, 20000)  # æœ€å¤šåˆ†æ2ä¸‡
    print(f"\nAnalyzing {analysis_size} patients...")

    all_qualities = analyzer.analyze_all(sample_size=analysis_size)

    # æ„å»ºæ ¸å¿ƒæ•°æ®é›†
    builder = CoreDatasetBuilder(analyzer)
    selected = builder.select_episodes(all_qualities, target_size=TARGET_EPISODES, require_alignment=True)

    # å¯¼å‡ºé€‰æ‹©ç»“æœ
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    selection_file = OUTPUT_DIR / 'core_episode_selection.csv'
    builder.export_selection(selection_file)

    # æ‰“å°æœ€ç»ˆæ‘˜è¦
    summary = builder.get_selection_summary()
    print("\n" + "=" * 70)
    print("Final Selection Summary")
    print("=" * 70)
    print(f"Total selected: {summary.get('total_selected', 0)}")
    print(f"Quality score range: {summary.get('quality_score_range', [])}")
    print(f"Average quality score: {summary.get('avg_quality_score', 0)}")
    print(f"Average patterns: {summary.get('avg_patterns', 0)}")
    print(f"Average notes: {summary.get('avg_notes', 0)}")
    print(f"\nDisease distribution: {summary.get('disease_distribution', {})}")

    # æ ¸å¿ƒæŒ‡æ ‡ï¼šAlignmentç»Ÿè®¡
    align_stats = summary.get('alignment_stats', {})
    print(f"\nAlignment Statistics (æ ¸å¿ƒæŒ‡æ ‡):")
    print(f"   With alignment: {align_stats.get('with_alignment', 0)} ({align_stats.get('alignment_rate', 0)*100:.1f}%)")
    print(f"   Average alignments per episode: {align_stats.get('avg_alignments', 0)}")
    print(f"   Max alignments: {align_stats.get('max_alignments', 0)}")

    print("\nCore dataset selection complete!")
    print(f"Selection saved to: {selection_file}")


if __name__ == "__main__":
    main()
