"""
Build Final TIMELY-Bench Dataset
ä¸ºæ¯ä¸ªICU episodeåˆ›å»ºä¸€ä¸ªå®Œæ•´çš„ç»“æ„åŒ–è®°å½•

åŒ…å«å†…å®¹ï¼š
1. æ‚£è€…åŸºæœ¬ä¿¡æ¯å’Œconditions
2. æ—¶åºæ•°æ®ç‰‡æ®µï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
3. ä¸´åºŠç¬”è®°ç‰‡æ®µï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
4. æ£€æµ‹åˆ°çš„patternåŠå…¶å¯¹åº”çš„æ—¶åº/æ–‡æœ¬è¯æ®
5. Pattern annotations (SUPPORTIVE/CONTRADICTORY/AMBIGUOUS)
6. é¢„æµ‹æ ‡ç­¾
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import json
import os
from typing import Dict, List, Any, Optional
from collections import defaultdict
from datetime import datetime

from config import (
    COHORT_FILE, TIMESERIES_FILE, NOTE_TIME_FILE,
    PATTERN_DETECTION_DIR, TEMPORAL_ALIGNMENT_DIR, ROOT_DIR
)

# ==========================================
# é…ç½®
# ==========================================
PATTERN_DETECTION_FILE = PATTERN_DETECTION_DIR / 'detected_patterns_24h.csv'
ALIGNMENT_FILE = TEMPORAL_ALIGNMENT_DIR / 'temporal_textual_alignment.csv'
PATTERN_TEMPLATES_FILE = ROOT_DIR / 'documentation' / 'pattern_templates.json'

OUTPUT_DIR = ROOT_DIR / 'TIMELY_Bench_Dataset'

# ==========================================
# 1. åŠ è½½æ‰€æœ‰æ•°æ®
# ==========================================

def load_all_data():
    """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®"""
    
    print("Loading all data sources...")
    
    data = {}
    
    # 1. Cohortä¿¡æ¯
    print("   Loading cohort...")
    data['cohort'] = pd.read_csv(COHORT_FILE)
    data['cohort']['stay_id'] = data['cohort']['stay_id'].astype(int)
    print(f"      {len(data['cohort'])} patients")
    
    # 2. æ—¶åºæ•°æ®
    print("   Loading timeseries...")
    data['timeseries'] = pd.read_csv(TIMESERIES_FILE)
    data['timeseries']['stay_id'] = data['timeseries']['stay_id'].astype(int)
    print(f"      {len(data['timeseries'])} records")
    
    # 3. ä¸´åºŠç¬”è®°
    print("   Loading notes...")
    data['notes'] = pd.read_csv(NOTE_FILE)
    data['notes']['stay_id'] = pd.to_numeric(data['notes']['stay_id'], errors='coerce').fillna(-1).astype(int)
    # å¤„ç†åˆ—å
    if 'radiology_text' in data['notes'].columns:
        data['notes']['text'] = data['notes']['radiology_text']
    print(f"      {len(data['notes'])} notes")
    
    # 4. Patternæ£€æµ‹ç»“æœ
    print("   Loading pattern detections...")
    if os.path.exists(PATTERN_DETECTION_FILE):
        data['patterns'] = pd.read_csv(PATTERN_DETECTION_FILE)
        print(f"      {len(data['patterns'])} detections")
    else:
        data['patterns'] = pd.DataFrame()
        print("      No pattern detection file found")
    
    # 5. Alignmentç»“æœ
    print("   Loading alignments...")
    if os.path.exists(ALIGNMENT_FILE):
        data['alignments'] = pd.read_csv(ALIGNMENT_FILE)
        print(f"      {len(data['alignments'])} alignments")
    else:
        data['alignments'] = pd.DataFrame()
        print("      No alignment file found")
    
    # 6. Patternæ¨¡æ¿
    print("   Loading pattern templates...")
    if os.path.exists(PATTERN_TEMPLATES_FILE):
        with open(PATTERN_TEMPLATES_FILE, 'r') as f:
            data['templates'] = json.load(f)
        print(f"      {sum(len(v['patterns']) for v in data['templates'].values())} templates")
    else:
        data['templates'] = {}
        print("      No template file found")
    
    return data

# ==========================================
# 2. æ„å»ºCondition Graph
# ==========================================

def build_condition_graph():
    """æ„å»ºç–¾ç—…å…³ç³»å›¾"""
    
    # å®šä¹‰ç–¾ç—…ä¹‹é—´çš„ä¸´åºŠå…³ç³»
    condition_relationships = {
        "sepsis_to_aki": {
            "source": "sepsis",
            "target": "aki",
            "relationship": "can_cause",
            "mechanism": "Sepsis-induced hypoperfusion and nephrotoxicity"
        },
        "sepsis_to_ards": {
            "source": "sepsis",
            "target": "ards",
            "relationship": "can_cause",
            "mechanism": "Inflammatory cascade causing lung injury"
        },
        "aki_to_ards": {
            "source": "aki",
            "target": "ards",
            "relationship": "bidirectional",
            "mechanism": "Fluid overload and inflammatory mediators"
        },
        "shock_to_aki": {
            "source": "shock",
            "target": "aki",
            "relationship": "can_cause",
            "mechanism": "Renal hypoperfusion"
        },
        "shock_to_ards": {
            "source": "shock",
            "target": "ards",
            "relationship": "can_cause",
            "mechanism": "Ischemia-reperfusion injury"
        }
    }
    
    return condition_relationships

def get_patient_condition_graph(conditions: List[str], graph: Dict) -> List[Dict]:
    """è·å–ç‰¹å®šæ‚£è€…çš„ç–¾ç—…å…³ç³»å­å›¾"""
    
    nodes = []
    for cond in conditions:
        nodes.append({
            "id": cond,
            "type": "condition",
            "present": True
        })
    
    edges = []
    for edge_id, edge_data in graph.items():
        if edge_data['source'] in conditions and edge_data['target'] in conditions:
            edges.append({
                "source": edge_data['source'],
                "target": edge_data['target'],
                "relationship": edge_data['relationship'],
                "mechanism": edge_data['mechanism']
            })
    
    return {"nodes": nodes, "edges": edges}

# ==========================================
# 3. æå–æ—¶åºç‰‡æ®µ
# ==========================================

def extract_timeseries_for_episode(
    stay_id: int, 
    timeseries_df: pd.DataFrame,
    window_hours: int = 24
) -> Dict[str, Any]:
    """æå–episodeçš„æ—¶åºæ•°æ®"""
    
    patient_ts = timeseries_df[
        (timeseries_df['stay_id'] == stay_id) & 
        (timeseries_df['hour'] < window_hours)
    ].copy()
    
    if len(patient_ts) == 0:
        return {"hours": [], "features": {}}
    
    # æŒ‰å°æ—¶ç»„ç»‡
    hours = sorted(patient_ts['hour'].unique().tolist())
    
    # æå–å„ç‰¹å¾
    feature_cols = [c for c in patient_ts.columns if c not in ['stay_id', 'hour']]
    
    features = {}
    for col in feature_cols:
        # æå–æ—¶é—´åºåˆ—
        series = []
        for h in hours:
            hour_data = patient_ts[patient_ts['hour'] == h]
            if len(hour_data) > 0:
                val = hour_data[col].values[0]
                if pd.notna(val):
                    series.append({"hour": int(h), "value": float(val)})
        
        if series:
            features[col] = series
    
    return {
        "window_hours": window_hours,
        "hours_available": hours,
        "n_hours": len(hours),
        "features": features
    }

# ==========================================
# 4. æå–ç¬”è®°ç‰‡æ®µ
# ==========================================

def extract_notes_for_episode(
    stay_id: int,
    notes_df: pd.DataFrame,
    window_hours: int = 24
) -> List[Dict]:
    """æå–episodeçš„ä¸´åºŠç¬”è®°"""
    
    patient_notes = notes_df[
        (notes_df['stay_id'] == stay_id) &
        (notes_df['hour_offset'] >= 0) &
        (notes_df['hour_offset'] < window_hours)
    ].copy()
    
    if len(patient_notes) == 0:
        return []
    
    notes_list = []
    for _, row in patient_notes.iterrows():
        note_text = str(row.get('text', ''))[:1000]  # æˆªæ–­é•¿æ–‡æœ¬
        
        notes_list.append({
            "note_id": str(row.get('note_id', '')),
            "hour_offset": float(row['hour_offset']),
            "category": row.get('category', 'Radiology'),
            "text": note_text,
            "text_length": len(note_text)
        })
    
    return notes_list

# ==========================================
# 5. æå–Patternå’ŒAnnotations
# ==========================================

def extract_patterns_for_episode(
    stay_id: int,
    patterns_df: pd.DataFrame,
    alignments_df: pd.DataFrame
) -> Dict[str, Any]:
    """æå–episodeçš„patternæ£€æµ‹å’Œå¯¹é½ç»“æœ"""
    
    patient_patterns = patterns_df[patterns_df['stay_id'] == stay_id]
    patient_alignments = alignments_df[alignments_df['stay_id'] == stay_id] if len(alignments_df) > 0 else pd.DataFrame()
    
    # æŒ‰patternåˆ†ç»„
    pattern_summary = {}
    
    for pattern_name in patient_patterns['pattern_name'].unique():
        pattern_data = patient_patterns[patient_patterns['pattern_name'] == pattern_name]
        
        # æ£€æµ‹äº‹ä»¶
        detections = []
        for _, row in pattern_data.iterrows():
            detections.append({
                "hour": int(row['hour']),
                "value": float(row['value']),
                "severity": row['severity'],
                "disease": row['disease']
            })
        
        # å¯¹é½çš„æ–‡æœ¬
        text_alignments = []
        if len(patient_alignments) > 0:
            pattern_aligns = patient_alignments[patient_alignments['pattern_name'] == pattern_name]
            for _, row in pattern_aligns.iterrows():
                relevant_text = row.get('note_text_relevant', '')
                if relevant_text and len(str(relevant_text)) > 5:
                    text_alignments.append({
                        "pattern_hour": int(row['pattern_hour']),
                        "note_hour": float(row['note_hour']),
                        "time_delta": float(row['time_delta_hours']),
                        "relevant_text": str(relevant_text)[:500],
                        # é¢„ç•™æ ‡æ³¨å­—æ®µ
                        "annotation": None  # SUPPORTIVE/CONTRADICTORY/AMBIGUOUS/UNRELATED
                    })
        
        pattern_summary[pattern_name] = {
            "n_detections": len(detections),
            "detections": detections[:20],  # é™åˆ¶æ•°é‡
            "n_text_alignments": len(text_alignments),
            "text_alignments": text_alignments[:10],  # é™åˆ¶æ•°é‡
            "first_detection_hour": min(d['hour'] for d in detections) if detections else None,
            "severity_max": max(d['severity'] for d in detections) if detections else None
        }
    
    return pattern_summary

def get_referenced_templates(
    patterns: Dict[str, Any],
    templates: Dict
) -> Dict[str, Any]:
    """è·å–è¢«å¼•ç”¨çš„patternæ¨¡æ¿å…ƒæ•°æ®"""
    
    referenced = {}
    pattern_names = set(patterns.keys())
    
    for disease_key, disease_data in templates.items():
        for template in disease_data.get('patterns', []):
            if template['name'] in pattern_names:
                referenced[template['name']] = {
                    "disease": disease_data['disease'],
                    "clinical_standard": disease_data['clinical_standard'],
                    "type": template['type'],
                    "feature": template['feature'],
                    "threshold": template.get('threshold'),
                    "direction": template.get('direction'),
                    "description": template['description'],
                    "severity": template['severity'],
                    "unit": template.get('unit', '')
                }
    
    return referenced

# ==========================================
# 6. æ„å»ºå•ä¸ªEpisode
# ==========================================

def build_episode_record(
    stay_id: int,
    cohort_row: pd.Series,
    timeseries_df: pd.DataFrame,
    notes_df: pd.DataFrame,
    patterns_df: pd.DataFrame,
    alignments_df: pd.DataFrame,
    templates: Dict,
    condition_graph: Dict,
    window_hours: int = 24
) -> Dict[str, Any]:
    """æ„å»ºå•ä¸ªepisodeçš„å®Œæ•´è®°å½•"""
    
    # 1. åŸºæœ¬ä¿¡æ¯
    episode = {
        "episode_id": f"mimic4_{stay_id}",
        "stay_id": int(stay_id),
        "patient_id": int(cohort_row.get('subject_id', 0)),
        "hadm_id": int(cohort_row.get('hadm_id', 0)),
    }
    
    # 2. Conditions
    conditions = []
    if cohort_row.get('has_sepsis_final', 0) == 1:
        conditions.append("sepsis")
    if cohort_row.get('has_aki_final', 0) == 1:
        conditions.append("aki")
    if cohort_row.get('has_ards', 0) == 1:
        conditions.append("ards")
    if cohort_row.get('has_shock', 0) == 1:
        conditions.append("shock")
    
    episode["conditions"] = conditions
    episode["n_conditions"] = len(conditions)
    
    # 3. Condition Graph
    episode["condition_graph"] = get_patient_condition_graph(conditions, condition_graph)
    
    # 4. æ—¶åºæ•°æ®
    episode["timeseries"] = extract_timeseries_for_episode(stay_id, timeseries_df, window_hours)
    
    # 5. ç¬”è®°ç‰‡æ®µ
    episode["notes_spans"] = extract_notes_for_episode(stay_id, notes_df, window_hours)
    episode["n_notes"] = len(episode["notes_spans"])
    
    # 6. Patternæ£€æµ‹å’Œå¯¹é½
    patterns = extract_patterns_for_episode(stay_id, patterns_df, alignments_df)
    episode["pattern_detections"] = patterns
    episode["n_patterns_detected"] = len(patterns)
    
    # 7. å¼•ç”¨çš„æ¨¡æ¿å…ƒæ•°æ®
    episode["physiology_templates"] = get_referenced_templates(patterns, templates)
    
    # 8. Pattern Annotationsæ‘˜è¦
    episode["pattern_annotations"] = {
        "total_alignments": sum(p.get('n_text_alignments', 0) for p in patterns.values()),
        "annotated": 0,  # å¾…æ ‡æ³¨
        "supportive": 0,
        "contradictory": 0,
        "ambiguous": 0,
        "unrelated": 0
    }
    
    # 9. Labels
    episode["labels"] = {
        "mortality": int(cohort_row.get('label_mortality', 0)),
        "prolonged_los_7d": int(cohort_row.get('prolonged_los_7d', 0)),
        "readmission_30d": int(cohort_row.get('readmission_30d', 0)),
        "los_hours": float(cohort_row.get('los_hours', 0)) if pd.notna(cohort_row.get('los_hours')) else None,
        "aki_stage_max": int(cohort_row.get('aki_stage_max', 0)) if pd.notna(cohort_row.get('aki_stage_max')) else None,
    }
    
    # 10. å…ƒæ•°æ®
    episode["metadata"] = {
        "window_hours": window_hours,
        "data_source": "MIMIC-IV v3.1",
        "created_at": datetime.now().isoformat()
    }
    
    return episode

# ==========================================
# 7. æ‰¹é‡æ„å»ºæ•°æ®é›†
# ==========================================

def build_dataset(
    data: Dict,
    output_dir: str,
    max_episodes: Optional[int] = None,
    window_hours: int = 24
) -> Dict[str, Any]:
    """æ„å»ºå®Œæ•´çš„TIMELY-Benchæ•°æ®é›†"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print("\nğŸ”¨ Building TIMELY-Bench Dataset...")
    print("=" * 60)
    
    # æ„å»ºcondition graph
    condition_graph = build_condition_graph()
    
    # è·å–æ‰€æœ‰stay_ids
    stay_ids = data['cohort']['stay_id'].unique()
    if max_episodes:
        stay_ids = stay_ids[:max_episodes]
    
    print(f"   Processing {len(stay_ids)} episodes...")
    
    # æ„å»ºæ¯ä¸ªepisode
    episodes = []
    stats = defaultdict(int)
    
    for i, stay_id in enumerate(stay_ids):
        if (i + 1) % 5000 == 0:
            print(f"   Processed {i+1}/{len(stay_ids)} episodes...")
        
        # è·å–cohortè¡Œ
        cohort_row = data['cohort'][data['cohort']['stay_id'] == stay_id].iloc[0]
        
        # æ„å»ºepisode
        episode = build_episode_record(
            stay_id=stay_id,
            cohort_row=cohort_row,
            timeseries_df=data['timeseries'],
            notes_df=data['notes'],
            patterns_df=data['patterns'],
            alignments_df=data['alignments'],
            templates=data['templates'],
            condition_graph=condition_graph,
            window_hours=window_hours
        )
        
        episodes.append(episode)
        
        # ç»Ÿè®¡
        stats['total_episodes'] += 1
        stats['with_notes'] += 1 if episode['n_notes'] > 0 else 0
        stats['with_patterns'] += 1 if episode['n_patterns_detected'] > 0 else 0
        stats['with_alignments'] += 1 if episode['pattern_annotations']['total_alignments'] > 0 else 0
        for cond in episode['conditions']:
            stats[f'condition_{cond}'] += 1
        stats['mortality_positive'] += episode['labels']['mortality']
    
    print(f"\n   Built {len(episodes)} episodes")
    
    # ==========================================
    # ä¿å­˜æ•°æ®é›†
    # ==========================================
    
    # 1. å®Œæ•´æ•°æ®é›† (JSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªepisode)
    jsonl_path = os.path.join(output_dir, 'timely_bench_episodes.jsonl')
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for episode in episodes:
            f.write(json.dumps(episode, ensure_ascii=False) + '\n')
    print(f"\nSaved: {jsonl_path}")
    
    # 2. é‡‡æ ·æ•°æ®é›† (ç”¨äºæ£€æŸ¥å’Œå±•ç¤º)
    sample_path = os.path.join(output_dir, 'sample_episodes.json')
    sample_episodes = episodes[:100]  # å‰100ä¸ª
    with open(sample_path, 'w', encoding='utf-8') as f:
        json.dump(sample_episodes, f, indent=2, ensure_ascii=False)
    print(f"Saved: {sample_path}")
    
    # 3. æ•°æ®é›†ç»Ÿè®¡
    stats_dict = dict(stats)
    stats_dict['total_patterns'] = sum(e['n_patterns_detected'] for e in episodes)
    stats_dict['total_notes'] = sum(e['n_notes'] for e in episodes)
    stats_dict['total_alignments'] = sum(e['pattern_annotations']['total_alignments'] for e in episodes)
    
    stats_path = os.path.join(output_dir, 'dataset_stats.json')
    with open(stats_path, 'w') as f:
        json.dump(stats_dict, f, indent=2)
    print(f"Saved: {stats_path}")
    
    # 4. Condition Graph
    graph_path = os.path.join(output_dir, 'condition_graph.json')
    with open(graph_path, 'w') as f:
        json.dump(condition_graph, f, indent=2)
    print(f"Saved: {graph_path}")
    
    # ==========================================
    # æ‰“å°ç»Ÿè®¡
    # ==========================================
    
    print("\n" + "=" * 60)
    print("DATASET STATISTICS")
    print("=" * 60)
    
    print(f"\n[Episodes]")
    print(f"   Total: {stats['total_episodes']}")
    print(f"   With notes: {stats['with_notes']} ({stats['with_notes']/stats['total_episodes']*100:.1f}%)")
    print(f"   With patterns: {stats['with_patterns']} ({stats['with_patterns']/stats['total_episodes']*100:.1f}%)")
    print(f"   With alignments: {stats['with_alignments']} ({stats['with_alignments']/stats['total_episodes']*100:.1f}%)")
    
    print(f"\n[Conditions]")
    for key, val in stats_dict.items():
        if key.startswith('condition_'):
            cond_name = key.replace('condition_', '')
            print(f"   {cond_name}: {val} ({val/stats['total_episodes']*100:.1f}%)")
    
    print(f"\n[Labels]")
    print(f"   Mortality positive: {stats['mortality_positive']} ({stats['mortality_positive']/stats['total_episodes']*100:.1f}%)")
    
    print(f"\n[Totals]")
    print(f"   Patterns: {stats_dict['total_patterns']}")
    print(f"   Notes: {stats_dict['total_notes']}")
    print(f"   Alignments: {stats_dict['total_alignments']}")
    
    return stats_dict

# ==========================================
# 8. ç”Ÿæˆæ•°æ®é›†æ–‡æ¡£
# ==========================================

def generate_dataset_readme(output_dir: str, stats: Dict):
    """ç”Ÿæˆæ•°æ®é›†README"""
    
    readme = f"""# TIMELY-Bench Dataset

## Overview

TIMELY-Bench is a benchmark dataset for time-aligned fusion of clinical time-series and notes in MIMIC-IV.

## Dataset Structure

Each episode is a JSON object with the following structure:

```json
{{
  "episode_id": "mimic4_12345",
  "patient_id": 10001,
  "stay_id": 12345,
  "hadm_id": 20001,
  
  "conditions": ["sepsis", "aki"],
  "n_conditions": 2,
  
  "condition_graph": {{
    "nodes": [...],
    "edges": [...]
  }},
  
  "timeseries": {{
    "window_hours": 24,
    "hours_available": [0, 1, 2, ...],
    "features": {{
      "heart_rate": [{{"hour": 0, "value": 85}}, ...],
      "creatinine": [{{"hour": 2, "value": 1.2}}, ...],
      ...
    }}
  }},
  
  "notes_spans": [
    {{
      "note_id": "note_001",
      "hour_offset": 3.5,
      "category": "Radiology",
      "text": "Chest X-ray shows..."
    }},
    ...
  ],
  
  "pattern_detections": {{
    "tachycardia": {{
      "n_detections": 5,
      "detections": [{{"hour": 2, "value": 105, "severity": "mild"}}, ...],
      "text_alignments": [{{"relevant_text": "Patient tachycardic...", "annotation": null}}, ...]
    }},
    ...
  }},
  
  "physiology_templates": {{
    "tachycardia": {{
      "disease": "Sepsis",
      "clinical_standard": "Sepsis-3",
      "threshold": 90,
      "direction": "above",
      ...
    }},
    ...
  }},
  
  "pattern_annotations": {{
    "total_alignments": 15,
    "annotated": 0,
    "supportive": 0,
    "contradictory": 0,
    "ambiguous": 0,
    "unrelated": 0
  }},
  
  "labels": {{
    "mortality": 0,
    "prolonged_los_7d": 1,
    "readmission_30d": 0,
    "los_hours": 168.5,
    "aki_stage_max": 2
  }},
  
  "metadata": {{
    "window_hours": 24,
    "data_source": "MIMIC-IV v3.1",
    "created_at": "2025-01-15T10:30:00"
  }}
}}
```

## Statistics

| Metric | Value |
|--------|-------|
| Total Episodes | {stats.get('total_episodes', 'N/A')} |
| With Notes | {stats.get('with_notes', 'N/A')} |
| With Patterns | {stats.get('with_patterns', 'N/A')} |
| With Alignments | {stats.get('with_alignments', 'N/A')} |
| Total Patterns | {stats.get('total_patterns', 'N/A')} |
| Total Alignments | {stats.get('total_alignments', 'N/A')} |

## Files

- `timely_bench_episodes.jsonl` - Complete dataset (one episode per line)
- `sample_episodes.json` - Sample of 100 episodes for inspection
- `dataset_stats.json` - Dataset statistics
- `condition_graph.json` - Disease relationship graph

## Usage

```python
import json

# Load episodes
episodes = []
with open('timely_bench_episodes.jsonl', 'r') as f:
    for line in f:
        episodes.append(json.loads(line))

# Filter by condition
sepsis_episodes = [e for e in episodes if 'sepsis' in e['conditions']]

# Get patterns for an episode
for pattern_name, pattern_data in episodes[0]['pattern_detections'].items():
    print(f"{{pattern_name}}: {{pattern_data['n_detections']}} detections")
```

## License

This dataset is derived from MIMIC-IV and requires PhysioNet credentialing.

## Citation

```bibtex
@misc{{timely-bench,
  title={{TIMELY-Bench: A Benchmark for Time-Aligned Fusion of Clinical Time-Series and Notes}},
  author={{Wang, Haoyu}},
  year={{2025}},
  institution={{King's College London}}
}}
```
"""
    
    readme_path = os.path.join(output_dir, 'README.md')
    with open(readme_path, 'w') as f:
        f.write(readme)
    print(f"Saved: {readme_path}")

# ==========================================
# Main
# ==========================================

def main():
    print("Building TIMELY-Bench Dataset")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    data = load_all_data()
    
    # æ„å»ºæ•°æ®é›†
    stats = build_dataset(
        data=data,
        output_dir=OUTPUT_DIR,
        max_episodes=None,  # å¤„ç†æ‰€æœ‰episode
        window_hours=24
    )
    
    # ç”ŸæˆREADME
    generate_dataset_readme(OUTPUT_DIR, stats)
    
    print("\n" + "=" * 60)
    print("TIMELY-Bench Dataset Complete!")
    print("=" * 60)
    print(f"\nOutput directory: {OUTPUT_DIR}/")
    print("   - timely_bench_episodes.jsonl (complete dataset)")
    print("   - sample_episodes.json (100 samples)")
    print("   - dataset_stats.json")
    print("   - condition_graph.json")
    print("   - README.md")

if __name__ == "__main__":
    main()