"""
æ‰¹é‡æ„å»ºå®Œæ•´Episodeæ•°æ®é›†ï¼ˆå…¨éƒ¨ 74,807 ä¸ª stay_idï¼‰
ä½¿ç”¨å®Œæ•´ 47GB å¯¹é½æ•°æ®å’Œ 30 workers

åŸºäº batch_build_core_episodes.py ä¿®æ”¹
"""

import pandas as pd
import json
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
from multiprocessing import Pool, Manager, cpu_count
from datetime import datetime
import logging
import sys
import time

# å¯¼å…¥ç°æœ‰çš„ Builder å’Œ Enhancer
from episode_builder import EpisodeBuilder, NumpyEncoder
from episode_enhancer import EpisodeEnhancer

# ==========================================
# é…ç½®
# ==========================================

_SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = _SCRIPT_DIR.parent.parent

EPISODES_DIR = PROJECT_ROOT / 'episodes'
EPISODES_ALL_DIR = EPISODES_DIR / 'episodes_all'  # æ–°çš„è¾“å‡ºç›®å½•
LOG_FILE = EPISODES_ALL_DIR / 'batch_build_all.log'
FAILED_IDS_FILE = EPISODES_ALL_DIR / 'failed_stay_ids.txt'

# ä» cohort è·å–æ‰€æœ‰ stay_ids
sys.path.insert(0, str(_SCRIPT_DIR.parent))
from config import COHORT_FILE, TEMPORAL_ALIGNMENT_DIR

# å¼ºåˆ¶ä½¿ç”¨å®Œæ•´å¯¹é½æ–‡ä»¶ï¼ˆ47GBï¼‰
FULL_ALIGNMENT_FILE = TEMPORAL_ALIGNMENT_DIR / 'temporal_textual_alignment.csv'

# ==========================================
# æ—¥å¿—é…ç½®
# ==========================================

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    EPISODES_ALL_DIR.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


# ==========================================
# å…¨å±€å˜é‡
# ==========================================

_global_builder = None
_global_enhancer = None


def init_worker_full():
    """åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹ï¼šåŠ è½½å®Œæ•´æ•°æ®åˆ°å…¨å±€å˜é‡"""
    global _global_builder, _global_enhancer
    
    # åˆ›å»º builderï¼Œç¦ç”¨ç´¢å¼•æ¨¡å¼ä»¥ç›´æ¥åŠ è½½å®Œæ•´æ–‡ä»¶
    _global_builder = EpisodeBuilder(use_alignment_index=False)
    _global_enhancer = EpisodeEnhancer()
    
    # åŠ è½½æ•°æ®æ—¶å¼ºåˆ¶ä½¿ç”¨å®Œæ•´å¯¹é½æ–‡ä»¶
    print(f"Worker {os.getpid()}: Loading data...")
    _global_builder.load_all_data_full()  # ä½¿ç”¨æ–°æ–¹æ³•
    _global_enhancer.aligner.load_data()
    print(f"Worker {os.getpid()}: Data loaded")


def process_single_stay_id_full(args: Tuple) -> Dict:
    """å¤„ç†å•ä¸ª stay_idï¼ˆä½¿ç”¨å…¨å±€ builderï¼‰"""
    global _global_builder, _global_enhancer
    
    stay_id, target_dir, force_rebuild = args
    
    result = {
        'stay_id': stay_id,
        'status': 'unknown',
        'message': '',
        'method': ''
    }
    
    try:
        target_file = target_dir / f"TIMELY_v2_{stay_id}.json"
        
        if not force_rebuild and target_file.exists():
            result['status'] = 'skipped'
            result['method'] = 'already_exists'
            return result
        
        # æ„å»º Episode
        episode = _global_builder.build_episode(stay_id)
        
        if episode is None:
            result['status'] = 'failed'
            result['method'] = 'build'
            result['message'] = 'No data found'
            return result
        
        # è½¬æ¢ä¸ºå­—å…¸
        episode_dict = episode.to_dict()
        
        # å¢å¼º Episode
        enhanced_dict = _global_enhancer.enhance_episode(episode_dict)
        
        # ä¿å­˜
        with open(target_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_dict, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        result['status'] = 'success'
        result['method'] = 'build_enhance'
        
    except Exception as e:
        result['status'] = 'failed'
        result['message'] = str(e)
    
    return result


def main(n_workers: int = 30, max_episodes: Optional[int] = None, force_rebuild: bool = False):
    """ä¸»å¤„ç†æµç¨‹"""
    import os
    global os  # è®© init_worker å¯ä»¥ä½¿ç”¨
    
    logger = setup_logging()
    
    print("=" * 80)
    print("æ‰¹é‡æ„å»ºå®Œæ•´Episodeæ•°æ®é›† (å…¨éƒ¨ 74,807 stay_ids)")
    print("=" * 80)
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Workers: {n_workers}")
    print(f"Alignment file: {FULL_ALIGNMENT_FILE}")
    print()
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    EPISODES_ALL_DIR.mkdir(parents=True, exist_ok=True)
    
    # è¯»å–æ‰€æœ‰ stay_ids
    cohort_df = pd.read_csv(COHORT_FILE)
    stay_ids = cohort_df['stay_id'].tolist()
    
    if max_episodes:
        stay_ids = stay_ids[:max_episodes]
        logger.info(f"Limited to first {max_episodes} episodes for testing")
    
    logger.info(f"Total stay_ids: {len(stay_ids)}")
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶
    if not force_rebuild:
        existing_files = set()
        for f in EPISODES_ALL_DIR.glob('TIMELY_v2_*.json'):
            try:
                sid = int(f.stem.replace('TIMELY_v2_', ''))
                existing_files.add(sid)
            except:
                pass
        logger.info(f"Already processed: {len(existing_files)}")
        stay_ids = [sid for sid in stay_ids if sid not in existing_files]
    
    if not stay_ids:
        logger.info("All episodes already processed!")
        return
    
    logger.info(f"Need to process: {len(stay_ids)}")
    
    # å‡†å¤‡å‚æ•°
    args_list = [(sid, EPISODES_ALL_DIR, force_rebuild) for sid in stay_ids]
    
    # å¹¶è¡Œå¤„ç†
    start_time = datetime.now()
    results = []
    
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨ init_worker_fullï¼Œå› ä¸ºæ¯ä¸ªè¿›ç¨‹éœ€è¦åŠ è½½ 47GB
    # æ›´å¥½çš„æ–¹å¼æ˜¯ä½¿ç”¨å…±äº«å†…å­˜æˆ–è€…é¡ºåºå¤„ç†
    # æš‚æ—¶ä½¿ç”¨é¡ºåºå¤„ç†æ–¹å¼ï¼Œæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½æ•°æ®
    
    print(f"\nğŸ”„ Processing {len(stay_ids)} episodes...")
    print("   Note: Loading 47GB alignment data (~6 min per worker initialization)...")
    
    with Pool(processes=n_workers, initializer=init_worker_full) as pool:
        for result in tqdm(
            pool.imap(process_single_stay_id_full, args_list),
            total=len(stay_ids),
            desc="Processing"
        ):
            results.append(result)
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    # ç»Ÿè®¡
    stats = {
        'total': len(args_list),
        'success': sum(1 for r in results if r['status'] == 'success'),
        'failed': sum(1 for r in results if r['status'] == 'failed'),
        'skipped': sum(1 for r in results if r['status'] == 'skipped'),
        'duration': duration
    }
    
    # è®°å½•å¤±è´¥
    failed = [r for r in results if r['status'] == 'failed']
    if failed:
        with open(FAILED_IDS_FILE, 'w') as f:
            for r in failed:
                f.write(f"{r['stay_id']}\t{r['message']}\n")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("å¤„ç†æ‘˜è¦")
    print("=" * 80)
    print(f"æ€»æ•°: {stats['total']}")
    print(f"æˆåŠŸ: {stats['success']} ({stats['success']/max(stats['total'],1)*100:.1f}%)")
    print(f"å¤±è´¥: {stats['failed']}")
    print(f"è·³è¿‡: {stats['skipped']}")
    print(f"è€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
    if stats['total'] > 0:
        print(f"é€Ÿåº¦: {stats['duration']/stats['total']:.2f}ç§’/episode")
    print(f"è¾“å‡º: {EPISODES_ALL_DIR}")
    print("=" * 80)
    
    logger.info(f"Complete: {stats['success']}/{stats['total']} success, {duration:.1f}s")


if __name__ == "__main__":
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description='æ„å»ºå®Œæ•´ Episode æ•°æ®é›†')
    parser.add_argument('--workers', type=int, default=30, help='è¿›ç¨‹æ•°')
    parser.add_argument('--max', type=int, default=None, help='æœ€å¤§å¤„ç†æ•°é‡')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡å»º')
    
    args = parser.parse_args()
    
    try:
        main(n_workers=args.workers, max_episodes=args.max, force_rebuild=args.force)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
