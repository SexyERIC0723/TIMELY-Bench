"""
æ‰¹é‡æ„å»ºæ ¸å¿ƒEpisodeæ•°æ®é›†
å¤„ç† core_episode_selection.csv ä¸­é€‰å®šçš„ 3000 ä¸ª stay_id

åŠŸèƒ½ï¼š
1. è¯»å–å·²é€‰å®šçš„ 3000 ä¸ª stay_id
2. æ£€æŸ¥ episodes_enhanced/ ä¸­æ˜¯å¦å·²å­˜åœ¨
3. å¦‚æœå­˜åœ¨åˆ™å¤åˆ¶ï¼›ä¸å­˜åœ¨åˆ™è°ƒç”¨ builder + enhancer ç”Ÿæˆ
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿›åº¦è·Ÿè¸ªã€é”™è¯¯è®°å½•

è®¾è®¡ï¼š
- ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿå¤„ç†
- æ˜¾ç¤ºè¯¦ç»†è¿›åº¦æ¡
- è®°å½•å¤±è´¥çš„ stay_id åˆ°æ—¥å¿—
- è¾“å‡ºå®Œæ•´ç»Ÿè®¡ä¿¡æ¯
"""

import pandas as pd
import json
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
from multiprocessing import Pool, Manager, cpu_count
from datetime import datetime
import logging
import sys

# å¯¼å…¥ç°æœ‰çš„ Builder å’Œ Enhancer
from episode_builder import EpisodeBuilder, NumpyEncoder
from episode_enhancer import EpisodeEnhancer

# ==========================================
# é…ç½®
# ==========================================

# è„šæœ¬æ‰€åœ¨ç›®å½•
_SCRIPT_DIR = Path(__file__).parent
# é¡¹ç›®æ ¹ç›®å½• (TIMELY-Bench_Final)
PROJECT_ROOT = _SCRIPT_DIR.parent.parent

# Episodesç›®å½•åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹
EPISODES_DIR = PROJECT_ROOT / 'episodes'
CORE_SELECTION_CSV = EPISODES_DIR / 'episodes_core' / 'core_episode_selection.csv'
EPISODES_ENHANCED_DIR = EPISODES_DIR / 'episodes_enhanced'
EPISODES_CORE_DIR = EPISODES_DIR / 'episodes_core'
LOG_FILE = EPISODES_CORE_DIR / 'batch_build.log'
FAILED_IDS_FILE = EPISODES_CORE_DIR / 'failed_stay_ids.txt'

# è¿›ç¨‹æ•°ï¼ˆç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿï¼‰
N_WORKERS = max(1, cpu_count() - 1)

# ==========================================
# æ—¥å¿—é…ç½®
# ==========================================

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


# ==========================================
# æ ¸å¿ƒå¤„ç†é€»è¾‘
# ==========================================

def check_existing_file(stay_id: int, source_dir: Path, target_dir: Path) -> Optional[Path]:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨

    Returns:
        å¦‚æœæ–‡ä»¶å­˜åœ¨è¿”å›æºæ–‡ä»¶è·¯å¾„ï¼Œå¦åˆ™è¿”å› None
    """
    filename = f"TIMELY_v2_{stay_id}.json"

    # 1. å…ˆæ£€æŸ¥ç›®æ ‡ç›®å½•ï¼ˆepisodes_coreï¼‰æ˜¯å¦å·²æœ‰
    target_file = target_dir / filename
    if target_file.exists():
        return None  # å·²å¤„ç†ï¼Œè·³è¿‡

    # 2. æ£€æŸ¥æºç›®å½•ï¼ˆepisodes_enhancedï¼‰æ˜¯å¦å­˜åœ¨
    source_file = source_dir / filename
    if source_file.exists():
        return source_file

    return None


# å…¨å±€å˜é‡ç”¨äºè¿›ç¨‹é—´å…±äº«å·²åŠ è½½çš„æ•°æ®
_global_builder = None
_global_enhancer = None


def init_worker():
    """åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹ï¼šåŠ è½½æ•°æ®åˆ°å…¨å±€å˜é‡"""
    global _global_builder, _global_enhancer
    _global_builder = EpisodeBuilder()
    _global_enhancer = EpisodeEnhancer()
    _global_builder.load_all_data()
    _global_enhancer.aligner.load_data()


def process_single_stay_id(args: Tuple, builder=None, enhancer=None, force_rebuild=False) -> Dict:
    """
    å¤„ç†å•ä¸ª stay_id

    Args:
        args: (stay_id, source_dir, target_dir) æˆ– (stay_id, source_dir, target_dir, force_rebuild)
        builder: å¯é€‰çš„é¢„åŠ è½½builderï¼ˆé¡ºåºæ¨¡å¼ï¼‰
        enhancer: å¯é€‰çš„é¢„åŠ è½½enhancerï¼ˆé¡ºåºæ¨¡å¼ï¼‰
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆ

    Returns:
        ç»“æœå­—å…¸ï¼š{'stay_id', 'status', 'message', 'method'}
    """
    global _global_builder, _global_enhancer

    # æ”¯æŒä¸¤ç§ args æ ¼å¼
    if len(args) == 4:
        stay_id, source_dir, target_dir, force_rebuild = args
    else:
        stay_id, source_dir, target_dir = args

    result = {
        'stay_id': stay_id,
        'status': 'unknown',
        'message': '',
        'method': ''
    }

    try:
        # å¦‚æœforce_rebuildï¼Œè·³è¿‡å·²å­˜åœ¨æ£€æŸ¥ï¼Œç›´æ¥æ„å»º
        if force_rebuild:
            # å¼ºåˆ¶é‡æ–°æ„å»º
            pass  # ç›´æ¥è¿›å…¥æ„å»ºæµç¨‹
        else:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_file = check_existing_file(stay_id, source_dir, target_dir)

            if existing_file:
                # æ–¹æ³•1ï¼šç›´æ¥å¤åˆ¶
                target_file = target_dir / existing_file.name
                shutil.copy2(existing_file, target_file)
                result['status'] = 'success'
                result['method'] = 'copy'
                result['message'] = f'Copied from {existing_file.name}'
                return result

            elif (target_dir / f"TIMELY_v2_{stay_id}.json").exists():
                # å·²ç»å¤„ç†è¿‡äº†
                result['status'] = 'skipped'
                result['method'] = 'already_exists'
                result['message'] = 'Already exists in target directory'
                return result

        # æ–¹æ³•2ï¼šéœ€è¦æ„å»º
        # ä½¿ç”¨ä¼ å…¥çš„æˆ–å…¨å±€çš„ builder/enhancer
        b = builder if builder else _global_builder
        e = enhancer if enhancer else _global_enhancer

        # æ„å»ºåŸºç¡€ Episode
        episode = b.build_episode(stay_id)

        if episode is None:
            result['status'] = 'failed'
            result['method'] = 'build'
            result['message'] = 'Failed to build episode (no data found)'
            return result

        # è½¬æ¢ä¸ºå­—å…¸
        episode_dict = episode.to_dict()

        # å¢å¼º Episode
        enhanced_dict = e.enhance_episode(episode_dict)

        # ä¿å­˜åˆ°ç›®æ ‡ç›®å½•
        target_file = target_dir / f"TIMELY_v2_{stay_id}.json"
        with open(target_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_dict, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)

        result['status'] = 'success'
        result['method'] = 'build_enhance'
        result['message'] = f'Built and enhanced from scratch{" (force rebuild)" if force_rebuild else ""}'

    except Exception as e:
        result['status'] = 'failed'
        result['message'] = str(e)

    return result


def process_single_stay_id_parallel(args: Tuple) -> Dict:
    """å¹¶è¡Œæ¨¡å¼ä¸‹çš„åŒ…è£…å‡½æ•°ï¼Œä½¿ç”¨å…¨å±€çš„builder/enhancer"""
    return process_single_stay_id(args)


def process_batch_sequential(stay_ids: List[int], source_dir: Path,
                            target_dir: Path, force_rebuild: bool = False) -> List[Dict]:
    """
    é¡ºåºå¤„ç†ï¼ˆç”¨äºè°ƒè¯•æˆ–å•è¿›ç¨‹æ¨¡å¼ï¼‰
    ä¼˜åŒ–ï¼šåªåŠ è½½ä¸€æ¬¡æ•°æ®ï¼Œå¤ç”¨builderå’Œenhancer
    """
    results = []

    # åˆå§‹åŒ– builder å’Œ enhancerï¼ˆé¡ºåºæ¨¡å¼ä¸‹å…±äº«ï¼‰
    builder = EpisodeBuilder()
    enhancer = EpisodeEnhancer()

    print("Loading data once (optimized sequential mode)...")
    builder.load_all_data()
    enhancer.aligner.load_data()

    print(f"\nğŸ”„ Processing {len(stay_ids)} episodes (sequential mode)...")
    if force_rebuild:
        print("   Force rebuild mode: will overwrite existing files")

    for stay_id in tqdm(stay_ids, desc="Processing"):
        args = (stay_id, source_dir, target_dir)
        # ä¼ å…¥é¢„åŠ è½½çš„builderå’Œenhancerï¼Œä»¥åŠforce_rebuildå‚æ•°
        result = process_single_stay_id(args, builder=builder, enhancer=enhancer, force_rebuild=force_rebuild)
        results.append(result)

    return results


def process_batch_parallel(stay_ids: List[int], source_dir: Path,
                          target_dir: Path, n_workers: int, force_rebuild: bool = False) -> List[Dict]:
    """
    å¹¶è¡Œå¤„ç†ï¼ˆå¤šè¿›ç¨‹ï¼‰
    ä¼˜åŒ–ï¼šæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ•°æ®
    """
    print(f"\nğŸ”„ Processing {len(stay_ids)} episodes with {n_workers} workers...")
    print(f"   Each worker will load data once at initialization...")
    if force_rebuild:
        print("   Force rebuild mode: will overwrite existing files")

    # å‡†å¤‡å‚æ•° - åŒ…å«force_rebuild
    args_list = [(stay_id, source_dir, target_dir, force_rebuild) for stay_id in stay_ids]

    # ä½¿ç”¨è¿›ç¨‹æ± ï¼ŒæŒ‡å®šåˆå§‹åŒ–å‡½æ•°
    results = []
    with Pool(processes=n_workers, initializer=init_worker) as pool:
        # ä½¿ç”¨ imap ä»¥æ”¯æŒè¿›åº¦æ¡
        for result in tqdm(
            pool.imap(process_single_stay_id_parallel, args_list),
            total=len(stay_ids),
            desc="Processing"
        ):
            results.append(result)

    return results


# ==========================================
# ä¸»æµç¨‹
# ==========================================

def main(use_parallel: bool = True, max_episodes: Optional[int] = None, force_rebuild: bool = False):
    """
    ä¸»å¤„ç†æµç¨‹

    Args:
        use_parallel: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ
        max_episodes: æœ€å¤§å¤„ç†æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰Episodeï¼ˆå¿½ç•¥å·²å­˜åœ¨æ–‡ä»¶ï¼‰
    """
    logger = setup_logging()

    print("=" * 80)
    print("æ‰¹é‡æ„å»ºæ ¸å¿ƒEpisodeæ•°æ®é›†")
    print("=" * 80)
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Workers: {N_WORKERS if use_parallel else 1}")
    print()

    # åˆ›å»ºç›®æ ‡ç›®å½•
    EPISODES_CORE_DIR.mkdir(parents=True, exist_ok=True)

    # è¯»å–é€‰å®šçš„ stay_ids
    logger.info(f"Reading stay_ids from {CORE_SELECTION_CSV}")
    if not CORE_SELECTION_CSV.exists():
        logger.error(f"Selection file not found: {CORE_SELECTION_CSV}")
        return

    df = pd.read_csv(CORE_SELECTION_CSV)
    stay_ids = df['stay_id'].tolist()

    if max_episodes:
        stay_ids = stay_ids[:max_episodes]
        logger.info(f"Limited to first {max_episodes} episodes for testing")

    logger.info(f"Found {len(stay_ids)} stay_ids to process")

    # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    existing_files = list(EPISODES_CORE_DIR.glob('TIMELY_v2_*.json'))
    existing_stay_ids = set()
    
    if force_rebuild:
        logger.info(f"Force rebuild enabled - will regenerate all {len(stay_ids)} episodes")
        # æ¸…ç©ºå·²å­˜åœ¨æ–‡ä»¶åˆ—è¡¨ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
        existing_stay_ids = set()
    else:
        for f in existing_files:
            try:
                stay_id = int(f.stem.replace('TIMELY_v2_', ''))
                existing_stay_ids.add(stay_id)
            except:
                pass
        logger.info(f"Found {len(existing_stay_ids)} already processed episodes")

    # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„ stay_ids
    stay_ids_to_process = [sid for sid in stay_ids if sid not in existing_stay_ids]

    if not stay_ids_to_process:
        logger.info("All episodes already processed!")
        print_summary({}, len(stay_ids), 0)
        return

    logger.info(f"Need to process {len(stay_ids_to_process)} episodes")

    # å¤„ç†
    start_time = datetime.now()

    if use_parallel:
        results = process_batch_parallel(
            stay_ids_to_process,
            EPISODES_ENHANCED_DIR,
            EPISODES_CORE_DIR,
            N_WORKERS,
            force_rebuild=force_rebuild
        )
    else:
        results = process_batch_sequential(
            stay_ids_to_process,
            EPISODES_ENHANCED_DIR,
            EPISODES_CORE_DIR,
            force_rebuild=force_rebuild
        )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    # ç»Ÿè®¡ç»“æœ
    stats = {
        'total': len(stay_ids_to_process),
        'success': sum(1 for r in results if r['status'] == 'success'),
        'failed': sum(1 for r in results if r['status'] == 'failed'),
        'skipped': sum(1 for r in results if r['status'] == 'skipped'),
        'copied': sum(1 for r in results if r['method'] == 'copy'),
        'built': sum(1 for r in results if r['method'] == 'build_enhance'),
        'duration': duration
    }

    # è®°å½•å¤±è´¥çš„ stay_ids
    failed_results = [r for r in results if r['status'] == 'failed']
    if failed_results:
        with open(FAILED_IDS_FILE, 'w', encoding='utf-8') as f:
            for r in failed_results:
                f.write(f"{r['stay_id']}\t{r['message']}\n")
        logger.warning(f"Failed stay_ids saved to: {FAILED_IDS_FILE}")

    # æ‰“å°æ‘˜è¦
    print_summary(stats, len(stay_ids), len(existing_stay_ids))

    # æ—¥å¿—è®°å½•
    logger.info("=" * 80)
    logger.info("Processing complete")
    logger.info(f"Total: {stats['total']}, Success: {stats['success']}, Failed: {stats['failed']}")
    logger.info(f"Duration: {duration:.1f}s ({duration/60:.1f} minutes)")
    logger.info("=" * 80)


def print_summary(stats: Dict, total_target: int, already_done: int):
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("å¤„ç†æ‘˜è¦")
    print("=" * 80)

    print(f"\n[ç›®æ ‡ç»Ÿè®¡]")
    print(f"  ç›®æ ‡æ€»æ•°: {total_target}")
    print(f"  å·²å®Œæˆ: {already_done}")
    print(f"  æœ¬æ¬¡å¤„ç†: {stats.get('total', 0)}")

    if stats:
        print(f"\n[æœ¬æ¬¡å¤„ç†ç»“æœ]")
        print(f"  æˆåŠŸ: {stats['success']} ({stats['success']/max(stats['total'], 1)*100:.1f}%)")
        print(f"  å¤±è´¥: {stats['failed']} ({stats['failed']/max(stats['total'], 1)*100:.1f}%)")
        print(f"  è·³è¿‡: {stats['skipped']}")

        print(f"\n[å¤„ç†æ–¹å¼]")
        print(f"  å¤åˆ¶: {stats['copied']}")
        print(f"  æ„å»º: {stats['built']}")

        print(f"\n[æ€§èƒ½]")
        print(f"  æ€»è€—æ—¶: {stats['duration']:.1f}ç§’ ({stats['duration']/60:.1f}åˆ†é’Ÿ)")
        if stats['total'] > 0:
            print(f"  å¹³å‡é€Ÿåº¦: {stats['duration']/stats['total']:.2f}ç§’/episode")

    print(f"\n[è¾“å‡ºç›®å½•]")
    print(f"  {EPISODES_CORE_DIR}/")

    current_files = len(list(EPISODES_CORE_DIR.glob('TIMELY_v2_*.json')))
    print(f"\n[å½“å‰çŠ¶æ€]")
    print(f"  å·²å®Œæˆ: {current_files}/{total_target} episodes")
    print(f"  å®Œæˆç‡: {current_files/total_target*100:.1f}%")

    print("=" * 80)


# ==========================================
# å‘½ä»¤è¡Œå…¥å£
# ==========================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='æ‰¹é‡æ„å»ºæ ¸å¿ƒEpisodeæ•°æ®é›†')
    parser.add_argument('--sequential', action='store_true',
                       help='ä½¿ç”¨é¡ºåºå¤„ç†ï¼ˆå•è¿›ç¨‹ï¼Œç”¨äºè°ƒè¯•ï¼‰')
    parser.add_argument('--max', type=int, default=None,
                       help='æœ€å¤§å¤„ç†æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--workers', type=int, default=N_WORKERS,
                       help=f'è¿›ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š{N_WORKERS}ï¼‰')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰Episodeï¼ˆå¿½ç•¥å·²å­˜åœ¨æ–‡ä»¶ï¼‰')

    args = parser.parse_args()

    # æ›´æ–°è¿›ç¨‹æ•°
    if args.workers:
        N_WORKERS = args.workers

    # è¿è¡Œ
    try:
        main(use_parallel=not args.sequential, max_episodes=args.max, force_rebuild=args.force)
    except KeyboardInterrupt:
        print("\n\n ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦")
        print("å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­å¤„ç†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
    except Exception as e:
        print(f"\n\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
